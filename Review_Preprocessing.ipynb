{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark must be first found using findspark package\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# establish a spark session with 50 executors\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"yarn\") \\\n",
    "        .appName(\"testing\") \\\n",
    "        .config(\"spark.executor.instances\", \"50\") \\\n",
    "        .config(\"spark.executor.memory\",\"5g\") \\\n",
    "        .config(\"spark.driver.memory\",\"30g\") \\\n",
    "        .config(\"spark.executor.cores\",'1') \\\n",
    "        .config(\"spark.scheduler.mode\",\"FIFO\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", '4g') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when wanting to stop the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the review dataset\n",
    "review = spark.read.parquet('/yelp/review.parquet').repartition(300).cache()\n",
    "business = spark.read.parquet('/yelp/business.parquet').repartition(100).cache()\n",
    "users = spark.read.parquet('/yelp/users.parquet').repartition(200).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import greatest\n",
    "# generate column for total aggregated count of votes\n",
    "review = review.withColumn('totalvotes', review.cool+review.funny+review.useful)\n",
    "\n",
    "# select only the reviews where one has at least 50 %\n",
    "review = review.withColumn('max_vote', greatest(review.cool,review.funny, review.useful))\n",
    "review = review.withColumn('max_ratio', review.max_vote/review.totalvotes)\n",
    "review = review.where( (review.max_vote / review.totalvotes) >=0.4)\n",
    "\n",
    "# filter out reviews with less than 10 reviews\n",
    "review = review.where(review.totalvotes >= 10)\n",
    "review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import lemmatized files, change format\n",
    "lemmatized = spark.read.json(\"/yelp/flatten_ lemmatized.json\").repartition(150)\n",
    "lemma_pd = lemmatized.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized = spark.createDataFrame(lemma_pd).repartition(150).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# WORD2VEC fitting\n",
    "import time\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec\n",
    "start = time.time()\n",
    "\n",
    "# tokenize\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "dataset = tokenizer.transform(lemmatized)\n",
    "\n",
    "# drop original text column\n",
    "dataset = dataset.drop(\"text\")\n",
    "\n",
    "# Stop word removal\n",
    "stopremove = StopWordsRemover(inputCol='words',outputCol='cleaned')\n",
    "dataset = stopremove.transform(dataset)\n",
    "\n",
    "dataset = dataset.drop('words').repartition(300).cache()\n",
    "\n",
    "#fit a word2vec model \n",
    "word2Vec = Word2Vec(vectorSize=500, minCount=0, numPartitions=300, inputCol=\"cleaned\", outputCol=\"word2vec\")\n",
    "model = word2Vec.fit(dataset)\n",
    "dataset = model.transform(dataset).drop('cleaned')\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assignclass(row):\n",
    "    if row.funny / row.totalvotes == max_ratio:\n",
    "        return \"funny\"\n",
    "    if row.cool / row.totalvotes == max_ratio:\n",
    "        return \"cool\"\n",
    "    else: return \"useful\"\n",
    "\n",
    "temp = dataset.rdd.map(assignclass).toDF()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Concept of pipeline\n",
    "# Additionally how to cross validate\n",
    "logit = LogisticRegression(featuresCol='word2vec',labelCol='max_category')\n",
    "cart = DecisionTreeClassifier(featuresCol='word2vec',labelCol='max_category')\n",
    "gbt = GBTClassifier(featuresCol='word2vec',labelCol='max_category')\n",
    "rf = RandomForestClassifier(featuresCol='word2vec',labelCol=\"max_category\")\n",
    "\n",
    "paramGrid_logit = ParamGridBuilder() \\\n",
    "    .addGrid(logit.regParam, [0,0.01, 0.1]) \\\n",
    "    .build()\n",
    "    \n",
    "paramGrid_cart = ParamGridBuilder() \\\n",
    "    .addGrid(cart.maxDepth, [10,12,15]) \\\n",
    "    .build()\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [15]) \\\n",
    "    .addGrid(rf.numTrees, [100]) \\\n",
    "    .build()\n",
    "    \n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [10,12,15]) \\\n",
    "    .addGrid(gbt.stepSize,[0.01]) \\\n",
    "    .addGrid(gbt.maxIter,[20]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='target')\n",
    "    \n",
    "\n",
    "cv_logit = CrossValidator(estimator=logit,evaluator=evaluator,estimatorParamMaps=paramGrid_logit,numFolds=5)\n",
    "cv_cart = CrossValidator(estimator=cart,evaluator=evaluator,estimatorParamMaps=paramGrid_cart,numFolds=5)\n",
    "cv_gbt = CrossValidator(estimator=gbt,evaluator=evaluator,estimatorParamMaps=paramGrid_gbt,numFolds=5)\n",
    "cv_rf = CrossValidator(estimator=pipeline_rf, evaluator=evaluator, numFolds=5, estimatorParamMaps=paramGrid_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvmodel_logit = cv_logit.fit(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using PCA on the new data to understand variance\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=10, inputCol= \"word2vec\",outputCol=\"pca_text\")\n",
    "pca_model = pca.fit(dataset)\n",
    "pca_result = pca_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_model.explainedVariance.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#np.arange(len(result.columns))\n",
    "#result.columns\n",
    "plt.plot(np.arange(10), np.array(pca_model.explainedVariance.values))\n",
    "plt.title('Explained Variance - PCA')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_result_pd = pca_result.toPandas()\n",
    "pca_result_pd['maxcat'] = pca_result_pd[['cool','useful','funny']].idxmax(axis = 1 )\n",
    "pca_result_pd['maxcat'] = pca_result_pd['maxcat'].astype('category')\n",
    "pca_result_pd['maxcat_code'] = pca_result_pd['maxcat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(pca_result.pca_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_pca(row):\n",
    "    return tuple(row.pca_text.toArray().tolist())\n",
    "pca_result = pca_result.rdd.map(extract_pca).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_result_pd = pca_result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "tsne= TSNE(early_exaggeration=10,n_jobs=20)\n",
    "tsne_output = tsne.fit_transform(np.array(pca_result_pd.pca_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(tsne_output[:,0],tsne_output[:,1],, c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "\n",
    "ax.scatter(xs=result_pd._1, ys=result_pd._2, zs = result_pd._3)\n",
    "plt.title('3d Representation of Word2Vec Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pd['maxcat'] = data[['cool','useful','funny']].idxmax(axis = 1 )\n",
    "result_pd['maxcat'] = result_pd['maxcat'].astype('category')\n",
    "result_pd['maxcat_code'] = result_pd['maxcat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
